{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bert_FakeNews_Pytorch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"57e8c75b03fc4f94b43ee626fec33f9b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ab5b4f242939450ab667f365f63227e1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_65d0dd8fde7a47958c73a3c7d6139e1a","IPY_MODEL_2226a1a2242e4647b11f8160ccc7aeeb"]}},"ab5b4f242939450ab667f365f63227e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65d0dd8fde7a47958c73a3c7d6139e1a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a9c156283f134c11a8ae374dde1b2027","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":109540,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":109540,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cb6542f3f1fa4c1aaf5ee0e094c3b0a3"}},"2226a1a2242e4647b11f8160ccc7aeeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f173ac6b7da946ed86993733eebe09c5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 110k/110k [00:00&lt;00:00, 339kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_561c2a3524ce41888e30326b19027bf1"}},"a9c156283f134c11a8ae374dde1b2027":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cb6542f3f1fa4c1aaf5ee0e094c3b0a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f173ac6b7da946ed86993733eebe09c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"561c2a3524ce41888e30326b19027bf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a965aaf4a9e3434e927597ec5e4debd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_cc7126ca81a94f8aaf030ed7fe66657b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a91227a1ed924f69b39f19b2e3131620","IPY_MODEL_b6e95332103b4366bf368567cecb6eaa"]}},"cc7126ca81a94f8aaf030ed7fe66657b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a91227a1ed924f69b39f19b2e3131620":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1584e754e1f94e1b993c56bca3707cb5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_60d26808bf394149b6e91a9b28e3af41"}},"b6e95332103b4366bf368567cecb6eaa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ade966050994c478727eadc5c0fb68a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 165B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3c987e0cdc144abfa919d8e7e62b9762"}},"1584e754e1f94e1b993c56bca3707cb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"60d26808bf394149b6e91a9b28e3af41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ade966050994c478727eadc5c0fb68a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3c987e0cdc144abfa919d8e7e62b9762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"216be3c27786466f82c523424882edfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3f5302be71ba4f21a976d631d4ec3026","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_49844b07e8ff4f9189f177cf312b15dd","IPY_MODEL_63e90b4b3ac3429a80d7eaaac63d4b12"]}},"3f5302be71ba4f21a976d631d4ec3026":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"49844b07e8ff4f9189f177cf312b15dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7f70809da5544dce8300462c2cf03ff0","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":268943,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":268943,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0675061aff074711a3f8e44de2f8004b"}},"63e90b4b3ac3429a80d7eaaac63d4b12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e7fc96b2981e41418f582e2bae1a2d02","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 269k/269k [00:00&lt;00:00, 3.98MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0681cb6ae3404c7e9ca17b9f1f14b7e9"}},"7f70809da5544dce8300462c2cf03ff0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0675061aff074711a3f8e44de2f8004b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e7fc96b2981e41418f582e2bae1a2d02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0681cb6ae3404c7e9ca17b9f1f14b7e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"SuvwACzSDtvr"},"source":["https://ripshun.com/2020/11/26/%E5%AE%9E%E6%88%98-%E4%BD%BF%E7%94%A8bert%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB/"]},{"cell_type":"code","metadata":{"id":"yu_Wb6U9znYL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626313534728,"user_tz":-480,"elapsed":19595,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"d9fff598-38f8-4793-e583-dd677c09cafb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CUB_AuvGQU35","colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"status":"ok","timestamp":1626313539221,"user_tz":-480,"elapsed":4550,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"129330e8-d27d-48fc-e0c0-0af6aa189c49"},"source":["import pandas as pd\n","path_train = './drive/MyDrive/AI_&_EdgeComputing_Program/NLP/NLP專案/WSDM-testing dataset/train.csv'\n","path_test = './drive/MyDrive/AI_&_EdgeComputing_Program/NLP/NLP專案/WSDM-testing dataset/test.csv'\n","\n","# 文件地址：https://www.kaggle.com/c/fake-news-pair-classification-challenge/data\n","# 模型形式：BERT + Linear Classifier\n","df_train = pd.read_csv(path_train)\n","\n","#除空 dataframe 的masked 遮罩\n","empty_title = ((df_train['title2_zh'].isnull()) \\\n","              | (df_train['title1_zh'].isnull()) \\\n","              | (df_train['title2_zh'] == '') \\\n","              | (df_train['title2_zh'] == '0'))\n","df_train = df_train[~empty_title] #~相反\n","\n","# 去除過長樣本\n","MAX_LENGTH = 30\n","df_train = df_train[~(df_train.title1_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n","df_train = df_train[~(df_train.title2_zh.apply(lambda x : len(x)) > MAX_LENGTH)]\n","\n","# 只用 1% 的训练集，看看bert的强大\n","SAMPLE_FRAC = 0.01\n","df_train = df_train.sample(frac=SAMPLE_FRAC, random_state=6666)\n","\n","# 去除沒用列\n","df_train = df_train.reset_index()\n","df_train = df_train.loc[:, ['title1_zh', 'title2_zh', 'label']]\n","df_train.columns = ['text_a', 'text_b', 'label']\n","\n","# 將结果另存成 tsv 供 pytorch 使用\n","df_train.to_csv(\"train.tsv\", sep=\"\\t\", index=False)\n","\n","print(\"训练样本数量：\", len(df_train))\n","df_train.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["训练样本数量： 2657\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_a</th>\n","      <th>text_b</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>晚上吃苹果就成毒苹果了吗</td>\n","      <td>早上吃金苹果，晚上吃毒苹果，苹果真不能晚上吃吗？</td>\n","      <td>agreed</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>吃酱油会变黑？伤口会留疤？</td>\n","      <td>经常吃酱油会变黑，这件事终于有答案了！</td>\n","      <td>agreed</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>加湿器加自来水堪比雾霾</td>\n","      <td>华为金立OPPO：我们手机明年要涨价！网友：有小米就够了</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>有谁希望丧尸病毒爆发，外星人入侵，世界巨</td>\n","      <td>丧尸病毒爆发之后 逃上一个小岛是否是一个最好的方案</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>山药好吃又营养，这么做还能补充维C、降血压</td>\n","      <td>常吃这3种食物，把血液垃圾清理的一干二净，还能降低血压</td>\n","      <td>unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  text_a                        text_b      label\n","0           晚上吃苹果就成毒苹果了吗      早上吃金苹果，晚上吃毒苹果，苹果真不能晚上吃吗？     agreed\n","1          吃酱油会变黑？伤口会留疤？           经常吃酱油会变黑，这件事终于有答案了！     agreed\n","2            加湿器加自来水堪比雾霾  华为金立OPPO：我们手机明年要涨价！网友：有小米就够了  unrelated\n","3   有谁希望丧尸病毒爆发，外星人入侵，世界巨     丧尸病毒爆发之后 逃上一个小岛是否是一个最好的方案  unrelated\n","4  山药好吃又营养，这么做还能补充维C、降血压   常吃这3种食物，把血液垃圾清理的一干二净，还能降低血压  unrelated"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"hkj3XQFpkYyi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626313539224,"user_tz":-480,"elapsed":28,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"499f0659-1c5e-42ef-c562-ff189d7a2a0c"},"source":["df_train.label.value_counts() / len(df_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["unrelated    0.675574\n","agreed       0.292811\n","disagreed    0.031615\n","Name: label, dtype: float64"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"6Iu_M-lZlB2v","colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"status":"ok","timestamp":1626313541487,"user_tz":-480,"elapsed":1873,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"6d339579-2a21-4d67-d8c4-a82bea91d074"},"source":["df_test = pd.read_csv(path_test)\n","df_test = df_test.loc[:, [\"title1_zh\", \"title2_zh\", \"id\"]]\n","df_test.columns = [\"text_a\", \"text_b\", \"Id\"]\n","df_test.to_csv(\"test.tsv\", sep=\"\\t\", index=False)\n","\n","print(\"預測樣本數：\", len(df_test))\n","df_test.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["預測樣本數： 80126\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_a</th>\n","      <th>text_b</th>\n","      <th>Id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大</td>\n","      <td>辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？</td>\n","      <td>321187</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n","      <td>10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国</td>\n","      <td>321190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗</td>\n","      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n","      <td>321189</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n","      <td>被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！</td>\n","      <td>321193</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>萨达姆被捕后告诫美国的一句话，发人深思</td>\n","      <td>中国川贝枇杷膏在美国受到热捧？纯属谣言！</td>\n","      <td>321191</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            text_a                       text_b      Id\n","0  萨拉赫人气爆棚!埃及总统大选未参选获百万选票 现任总统压力山大  辟谣！里昂官方否认费基尔加盟利物浦，难道是价格没谈拢？  321187\n","1              萨达姆被捕后告诫美国的一句话，发人深思    10大最让美国人相信的荒诞谣言，如蜥蜴人掌控着美国  321190\n","2    萨达姆此项计划没有此国破坏的话，美国还会对伊拉克发动战争吗          萨达姆被捕后告诫美国的一句话，发人深思  321189\n","3              萨达姆被捕后告诫美国的一句话，发人深思  被绞刑处死的萨达姆是替身？他的此男人举动击破替身谣言！  321193\n","4              萨达姆被捕后告诫美国的一句话，发人深思         中国川贝枇杷膏在美国受到热捧？纯属谣言！  321191"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"-P8Qv8GSmlED","colab":{"base_uri":"https://localhost:8080/","height":303,"referenced_widgets":["57e8c75b03fc4f94b43ee626fec33f9b","ab5b4f242939450ab667f365f63227e1","65d0dd8fde7a47958c73a3c7d6139e1a","2226a1a2242e4647b11f8160ccc7aeeb","a9c156283f134c11a8ae374dde1b2027","cb6542f3f1fa4c1aaf5ee0e094c3b0a3","f173ac6b7da946ed86993733eebe09c5","561c2a3524ce41888e30326b19027bf1","a965aaf4a9e3434e927597ec5e4debd9","cc7126ca81a94f8aaf030ed7fe66657b","a91227a1ed924f69b39f19b2e3131620","b6e95332103b4366bf368567cecb6eaa","1584e754e1f94e1b993c56bca3707cb5","60d26808bf394149b6e91a9b28e3af41","8ade966050994c478727eadc5c0fb68a","3c987e0cdc144abfa919d8e7e62b9762","216be3c27786466f82c523424882edfa","3f5302be71ba4f21a976d631d4ec3026","49844b07e8ff4f9189f177cf312b15dd","63e90b4b3ac3429a80d7eaaac63d4b12","7f70809da5544dce8300462c2cf03ff0","0675061aff074711a3f8e44de2f8004b","e7fc96b2981e41418f582e2bae1a2d02","0681cb6ae3404c7e9ca17b9f1f14b7e9"]},"executionInfo":{"status":"ok","timestamp":1626313557993,"user_tz":-480,"elapsed":16564,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"56cc1af0-9ede-4974-fd5a-da24865aa363"},"source":["from torch.utils.data import Dataset\n","!pip install transformers tqdm boto3 requests regex -q\n","from transformers import BertTokenizer #詳見下一區塊，從huggingface github載入了BertTokenizer的 class information\n","!pip install pysnooper -q\n","import pysnooper\n","\n","PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  \n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)#導入模型huggingface的pytorch-pretrained-BERT\n","#Bert tokenizer class中\n","class FakeNewsDataset(Dataset):\n","    def __init__(self, mode, tokenizer):\n","        assert mode in [\"train\", \"test\"]\n","        self.mode = mode\n","        #iterator=True pd讀取大文件方法\n","        self.df = pd.read_csv(mode + \".tsv\", sep=\"\\t\").fillna(\"\")\n","        self.len = len(self.df)\n","        self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n","        self.tokenizer = tokenizer  #使用 BERT tokenizer\n","\n","    #@pysnooper.snoop()  # 加入以了解所有轉換過程\n","    def __getitem__(self, idx):\n","        if self.mode == \"test\": #若是test資料集\n","            text_a, text_b = self.df.iloc[idx, :2].values \n","            #呼叫self的df (test檔)，包含idx欄位，取出0,1兩欄，用values轉成list 再依序assign給text_a, text_b\n","            label_tensor = None #因為test 沒有label\n","\n","        else: #因為training dataset欄位已經處理過只剩text_a, text_b, label\n","                          #index數為idx，欄位全部取出(:) 並轉成List(values)\n","            text_a, text_b, label = self.df.iloc[idx, :].values\n","            label_id = self.label_map[label] #label承接訓練集資料值，並將其轉成數字\n","            #self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n","            \n","            label_tensor = torch.tensor(label_id) #將agreed, disagreed, unrelated 轉成張量\n","            #print(label_tensor) #label_tensor = tensor(0-2)\n","\n","        word_pieces = [\"[CLS]\"] #設定list 裝CLS的字串\n","        tokens_a = self.tokenizer.tokenize(text_a) #將text_a欄用pytorch 的tokenize斷詞，存成tokens_a \n","        word_pieces += tokens_a + [\"[SEP]\"] #tokens_a 變成 [CLS]內容[SEP]，暫存成word_pieces 變數\n","        len_a = len(word_pieces) #看word_pieces(tokens_a)的長度\n","        \n","        tokens_b = self.tokenizer.tokenize(text_b) #將text_b欄用pytorch 的tokenize斷詞，存成tokens_b \n","        word_pieces += tokens_b + [\"[SEP]\"] #tokens_b 變成 [CLS]內容[SEP]，暫存成word_pieces 變數\n","        len_b = len(word_pieces) - len_a  #word_pieces(tokens_b)的長度 - word_pieces(tokens_a)的長度\n","        #???\n","\n","        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n","        #使用def __getitem__(self, idx)中tokenizer的convert_tokens_to_ids的function，輸入word_pieces\n","        #此時word_pieces是啥? tokens_b? function 功能未知\n","\n","        tokens_tensor = torch.tensor(ids) #torch function 功能未知\n","        \n","        segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long)\n","        #torch function 功能未知\n","        return (tokens_tensor, segments_tensor, label_tensor)\n","    \n","    def __len__(self):\n","        return self.len\n","    \n","trainset = FakeNewsDataset(\"train\", tokenizer=tokenizer)\n","print(trainset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.5MB 28.8MB/s \n","\u001b[K     |████████████████████████████████| 133kB 53.8MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 47.5MB/s \n","\u001b[K     |████████████████████████████████| 901kB 51.4MB/s \n","\u001b[K     |████████████████████████████████| 7.7MB 33.6MB/s \n","\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n","\u001b[31mERROR: botocore 1.20.112 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","\u001b[?25h"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57e8c75b03fc4f94b43ee626fec33f9b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a965aaf4a9e3434e927597ec5e4debd9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"216be3c27786466f82c523424882edfa","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=268943.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","<__main__.FakeNewsDataset object at 0x7f15e5dd1a10>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LpbLXkDTaJ4q"},"source":["因為上個block有import 到pytorch_pretrained_bert.tokenization中的BertTokenizer.from_pretrained函数。打开pytorch_pretrained_bert源代码，BertTokenizer类如下：\n","\n","```\n","class BertTokenizer(object):\n","    \"\"\"Runs end-to-end tokenization: punctuation splitting + wordpiece\"\"\"\n","\n","    def __init__(self, vocab_file, do_lower_case=True, max_len=None,\n","                 never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n","        if not os.path.isfile(vocab_file):\n","            raise ValueError(\n","                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n","                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file))\n","        self.vocab = load_vocab(vocab_file)\n","        self.ids_to_tokens = collections.OrderedDict(\n","            [(ids, tok) for tok, ids in self.vocab.items()])\n","        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n","                                              never_split=never_split)\n","        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n","        self.max_len = max_len if max_len is not None else int(1e12)\n","\n","    def tokenize(self, text):\n","        split_tokens = []\n","        for token in self.basic_tokenizer.tokenize(text):\n","            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n","                split_tokens.append(sub_token)\n","        return split_tokens\n","\n","    def convert_tokens_to_ids(self, tokens):\n","        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n","        ids = []\n","        for token in tokens:\n","            ids.append(self.vocab[token])\n","        if len(ids) > self.max_len:\n","            raise ValueError(\n","                \"Token indices sequence length is longer than the specified maximum \"\n","                \" sequence length for this BERT model ({} > {}). Running this\"\n","                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n","            )\n","        return ids\n","\n","    def convert_ids_to_tokens(self, ids):\n","        \"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"\n","        tokens = []\n","        for i in ids:\n","            tokens.append(self.ids_to_tokens[i])\n","        return tokens\n"," \n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name, cache_dir=None, *inputs, **kwargs):\n","        \"\"\"\n","        Instantiate a PreTrainedBertModel from a pre-trained model file.\n","        Download and cache the pre-trained model file if needed.\n","        \"\"\"\n","        if pretrained_model_name in PRETRAINED_VOCAB_ARCHIVE_MAP:\n","            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name]\n","        else:\n","            vocab_file = pretrained_model_name\n","        if os.path.isdir(vocab_file):\n","            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n","        # redirect to the cache, if necessary\n","        try:\n","            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n","        except FileNotFoundError:\n","            logger.error(\n","                \"Model name '{}' was not found in model name list ({}). \"\n","                \"We assumed '{}' was a path or url but couldn't find any file \"\n","                \"associated to this path or url.\".format(\n","                    pretrained_model_name,\n","                    ', '.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n","                    vocab_file))\n","            return None\n","        if resolved_vocab_file == vocab_file:\n","            logger.info(\"loading vocabulary file {}\".format(vocab_file))\n","        else:\n","            logger.info(\"loading vocabulary file {} from cache at {}\".format(\n","                vocab_file, resolved_vocab_file))\n","        if pretrained_model_name in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n","            # if we're using a pretrained model, ensure the tokenizer wont index sequences longer\n","            # than the number of positional embeddings\n","            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name]\n","            kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n","        # Instantiate tokenizer.\n","        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n","        return tokenizer```\n","\n"]},{"cell_type":"code","metadata":{"id":"LHn8Yg4xpr2D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626313557997,"user_tz":-480,"elapsed":85,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"d286ae73-4309-48de-f631-354cde0e92e0"},"source":["import torch\n","sample_idx = 0\n","print(trainset) #去看上面的式子class FakeNewsDataset(Dataset): \n","#<__main__.FakeNewsDataset object at 0x7f43cc4cffd0>\n","\n","text_a, text_b, label = trainset.df.iloc[sample_idx].values # df.iloc [0]第0 row筆資料轉成list\n","tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx] \n","tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n","combined_text = \"\".join(tokens)\n","print(combined_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<__main__.FakeNewsDataset object at 0x7f15e5dd1a10>\n","[CLS]晚上吃苹果就成毒苹果了吗[SEP]早上吃金苹果，晚上吃毒苹果，苹果真不能晚上吃吗？[SEP]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ZNy9ADZp5Er"},"source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","\n","#此函式由collate_fn呼叫，基本功能是傳入tokenized的張量與segment embedding進來，並完成zero padding到單句256\n","def create_mini_batch(samples):\n","    tokens_tensors = [s[0] for s in samples] #samples 參數傳入, sample = 裝已經轉向量的句子\n","    #print('samples',samples)\n","    segments_tensors = [s[1] for s in samples]\n","    #print('segments_tensors',segments_tensors)\n","    \n","    if samples[0][2] is not None:\n","        label_ids = torch.stack([s[2] for s in samples]) #注意: torch.stack(dim =2)表示疊加在第3rd dim在第3rd dim\n","    else:\n","        label_ids = None\n","    \n","    # zero pad 到同一序列長度\n","    tokens_tensors = pad_sequence(tokens_tensors,batch_first=True)\n","    segments_tensors = pad_sequence(segments_tensors,batch_first=True)\n","    \n","    # attention masks，將 tokens_tensors 不為 zero padding 的位置設為1\n","    masks_tensors = torch.zeros(tokens_tensors.shape,dtype=torch.long) #torch.long = int 64\n","    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n","    \n","    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n","\n","BATCH_SIZE = 64\n","\n","\n","trainloader = DataLoader(trainset,batch_size=BATCH_SIZE,collate_fn=create_mini_batch) #一樣統一用batch 64 training data\n","#在此num_workers default = 0，若CPU能負荷，可開啟多線程，事先載入batch進RAM，training batch speed rising\n","#經驗上，預設為CPU核心數\n","# collate_fn上，我們參考論文與LeeMing前輩的文章，複製create_mini_batch function進來，自訂義我們batch的儲存方式\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ad4EtA18_pGG"},"source":["from transformers import BertForSequenceClassification\n","from IPython.display import clear_output\n","\n","# # Prepare model\n","PRETRAINED_MODEL_NAME = \"bert-base-chinese\" #從google裡抓出\"bert-base-chinese\"參數, config\n","NUM_LABELS = 3\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n","\n","clear_output()\n","#model.config"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CNgUDLphBcun"},"source":["#我們呼叫的BertForSequenceClassification\n","\n","1. 参数：\n","\n","    config：指定的bert模型的預訓練參數\n","\n","    num_labels：label (類別)的數量 (num)\n","2. 輸入：\n","\n","    input_ids：訓練集，torch.LongTensors(int 64)，shape是[batch_size,sequence_length]\n","\n","    **token_type_ids：optional，当训练集是两句话时才有**(Adopted)\n","\n","    **attention_mask：optional，当使用mask才有** (Adopted)\n","\n","    labels：Data labelled，torch.LongTensor類型，shape是[batch_size],同input_ids\n","\n","3. 輸出：\n","\n","    if labels != None（訓練時）：output = 是分類的crossentropy\n","\n","    if labels == None（評價用）：output = 機率 且shape為[batch_size, num_labels]\n","\n","```\n","#我們呼叫的BertForSequenceClassification\n","###BertForSequenceClassification class, code as below：\n","\n","\n","class BertForSequenceClassification(BertPreTrainedModel):\n","    def __init__(self, config, num_labels=2, ...):\n","        super(BertForSequenceClassification, self).__init__(config)\n","        self.num_labels = num_labels\n","        self.bert = BertModel(config, ...)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","          ...\n","\n","    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None, ...):\n","        outputs = self.bert(input_ids, token_type_ids, attention_mask, ...)\n","        ...\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            return loss\n","        elif self.output_attentions:\n","            return all_attentions, logits\n","        return logit\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"J_jfSbyLRCa_"},"source":["# PreTrainedBertModel\n","\n","(BertForSequenceClassification 為繼承自PreTrainedBertModel的子類)\n","\n","\n","\n","```\n","class PreTrainedBertModel(nn.Module):\n","    \"\"\" An abstract class to handle weights initialization and\n","        a simple interface for dowloading and loading pretrained models.\n","    \"\"\"\n","    def __init__(self, config, *inputs, **kwargs):\n","        super(PreTrainedBertModel, self).__init__()\n","        if not isinstance(config, BertConfig):\n","            raise ValueError(\n","                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n","                \"To create a model from a Google pretrained model use \"\n","                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n","                    self.__class__.__name__, self.__class__.__name__\n","                ))\n","        self.config = config\n","\n","    def init_bert_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name, state_dict=None, cache_dir=None, *inputs, **kwargs):\n","        \"\"\"\n","        参数:\n","            预训练模型名称，可选: \n","                    . `bert-base-uncased`\n","                    . `bert-large-uncased`\n","                    . `bert-base-cased`\n","                    . `bert-large-cased`\n","                    . `bert-base-multilingual-uncased`\n","                    . `bert-base-multilingual-cased`\n","                    . `bert-base-chinese\n","        \"\"\"\n","        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n","            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n","        else:\n","            archive_file = pretrained_model_name\n","        # redirect to the cache, if necessary\n","        try:\n","            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n","        except FileNotFoundError:\n","            logger.error(\n","                \"Model name '{}' was not found in model name list ({}). \"\n","                \"We assumed '{}' was a path or url but couldn't find any file \"\n","                \"associated to this path or url.\".format(\n","                    pretrained_model_name,\n","                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n","                    archive_file))\n","            return None\n","        if resolved_archive_file == archive_file:\n","            logger.info(\"loading archive file {}\".format(archive_file))\n","        else:\n","            logger.info(\"loading archive file {} from cache at {}\".format(\n","                archive_file, resolved_archive_file))\n","        tempdir = None\n","        if os.path.isdir(resolved_archive_file):\n","            serialization_dir = resolved_archive_file\n","        else:\n","            # Extract archive to temp dir\n","            tempdir = tempfile.mkdtemp()\n","            logger.info(\"extracting archive file {} to temp dir {}\".format(\n","                resolved_archive_file, tempdir))\n","            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n","                archive.extractall(tempdir)\n","            serialization_dir = tempdir\n","        # Load config\n","        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n","        config = BertConfig.from_json_file(config_file)\n","        logger.info(\"Model config {}\".format(config))\n","        # Instantiate model.\n","        model = cls(config, *inputs, **kwargs)\n","        if state_dict is None:\n","            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n","            state_dict = torch.load(weights_path)\n","\n","        old_keys = []\n","        new_keys = []\n","        for key in state_dict.keys():\n","            new_key = None\n","            if 'gamma' in key:\n","                new_key = key.replace('gamma', 'weight')\n","            if 'beta' in key:\n","                new_key = key.replace('beta', 'bias')\n","            if new_key:\n","                old_keys.append(key)\n","                new_keys.append(new_key)\n","        for old_key, new_key in zip(old_keys, new_keys):\n","            state_dict[new_key] = state_dict.pop(old_key)\n","\n","        missing_keys = []\n","        unexpected_keys = []\n","        error_msgs = []\n","        # copy state_dict so _load_from_state_dict can modify it\n","        metadata = getattr(state_dict, '_metadata', None)\n","        state_dict = state_dict.copy()\n","        if metadata is not None:\n","            state_dict._metadata = metadata\n","\n","        def load(module, prefix=''):\n","            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n","            module._load_from_state_dict(\n","                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n","            for name, child in module._modules.items():\n","                if child is not None:\n","                    load(child, prefix + name + '.')\n","        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n","        if len(missing_keys) > 0:\n","            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n","                model.__class__.__name__, missing_keys))\n","        if len(unexpected_keys) > 0:\n","            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n","                model.__class__.__name__, unexpected_keys))\n","        if tempdir:\n","            # Clean up temp dir\n","            shutil.rmtree(tempdir)\n","        return model\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"hXdwfRscAckW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626313592999,"user_tz":-480,"elapsed":22221,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"d55dd54f-e46a-4f8e-ed35-d16717f0b953"},"source":["def get_predictions(model, dataloader, compute_acc=False):\n","    predictions = None\n","    correct = 0\n","    total = 0\n","      \n","    with torch.no_grad(): #被torch.no_grad() wrapped的上下文不會被梯度下降\n","        for data in dataloader:\n","            if next(model.parameters()).is_cuda:\n","                data = [t.to(\"cuda:0\") for t in data if t is not None]\n","            \n","            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n","            outputs = model(input_ids=tokens_tensors,token_type_ids=segments_tensors,attention_mask=masks_tensors)\n","            \n","            logits = outputs[0]\n","            _, pred = torch.max(logits.data, 1)\n","            \n","            if compute_acc:\n","                labels = data[3]\n","                total += labels.size(0)\n","                correct += (pred == labels).sum().item()\n","                \n","            if predictions is None:\n","                predictions = pred\n","            else:\n","                predictions = torch.cat((predictions, pred))\n","    \n","    if compute_acc:\n","        acc = correct / total\n","        return predictions, acc\n","    return predictions\n","    \n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device:\", device)\n","model = model.to(device)\n","_, acc = get_predictions(model, trainloader, compute_acc=True)\n","print(\"classification acc:\", acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["device: cuda:0\n","classification acc: 0.5724501317275122\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t1cZmsyoCZtr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626314609137,"user_tz":-480,"elapsed":284537,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"4198cc51-00e9-4a38-fdfb-974f0f9e5ca5"},"source":["model.train()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5) #前期可設置大  後期(特定幾個epoch)可設小 \n","EPOCHS = 7\n","for epoch in range(EPOCHS):\n","    \n","    running_loss = 0.0\n","    for data in trainloader:\n","        \n","        tokens_tensors, segments_tensors, \\\n","        masks_tensors, labels = [t.to(device) for t in data]\n","        optimizer.zero_grad()\n","        outputs = model(input_ids=tokens_tensors, \n","                        token_type_ids=segments_tensors, \n","                        attention_mask=masks_tensors, \n","                        labels=labels)\n","\n","        loss = outputs[0]\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        \n","    _, acc = get_predictions(model, trainloader, compute_acc=True)\n","\n","    print('[epoch %d] loss: %.3f, acc: %.3f' %\n","          (epoch + 1, running_loss, acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[epoch 1] loss: 4.047, acc: 0.982\n","[epoch 2] loss: 3.284, acc: 0.982\n","[epoch 3] loss: 1.995, acc: 0.989\n","[epoch 4] loss: 1.595, acc: 0.992\n","[epoch 5] loss: 1.180, acc: 0.988\n","[epoch 6] loss: 1.059, acc: 0.988\n","[epoch 7] loss: 1.362, acc: 0.991\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hBpBJQ1NKiR2","colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"status":"ok","timestamp":1626314303907,"user_tz":-480,"elapsed":428633,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"e518e28d-28ce-495a-840a-f756eefa5916"},"source":["testset = FakeNewsDataset(\"test\", tokenizer=tokenizer)\n","testloader = DataLoader(testset, batch_size=256, \n","                        collate_fn=create_mini_batch)\n","\n","predictions = get_predictions(model, testloader)\n","index_map = {v: k for k, v in testset.label_map.items()}\n","\n","df = pd.DataFrame({\"Category\": predictions.tolist()})\n","df['Category'] = df.Category.apply(lambda x: index_map[x])\n","df_pred = pd.concat([testset.df.loc[:, [\"Id\"]], df.loc[:, 'Category']], axis=1)\n","df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n","df_pred.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>321187</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>321190</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>321189</td>\n","      <td>agreed</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>321193</td>\n","      <td>unrelated</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>321191</td>\n","      <td>unrelated</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Id   Category\n","0  321187  unrelated\n","1  321190  unrelated\n","2  321189     agreed\n","3  321193  unrelated\n","4  321191  unrelated"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"4SMmoJg_LFRr","colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"status":"ok","timestamp":1626314313780,"user_tz":-480,"elapsed":9915,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"07297545-5071-43d0-b40a-89fe09d0896b"},"source":["predictions = get_predictions(model, trainloader)\n","df = pd.DataFrame({\"predicted\": predictions.tolist()})\n","df['predicted'] = df.predicted.apply(lambda x: index_map[x])\n","df1 = pd.concat([trainset.df, df.loc[:, 'predicted']], axis=1)\n","disagreed_tp = ((df1.label == 'disagreed') & \\\n","                (df1.label == df1.predicted) & \\\n","                (df1.text_a.apply(lambda x: True if len(x) < 10 else False)))\n","df1[disagreed_tp].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text_a</th>\n","      <th>text_b</th>\n","      <th>label</th>\n","      <th>predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>25</th>\n","      <td>关于植物除甲醛</td>\n","      <td>医学博士拆穿，市面那些关于装修除甲醛谣言,毫无科学依据</td>\n","      <td>disagreed</td>\n","      <td>disagreed</td>\n","    </tr>\n","    <tr>\n","      <th>1605</th>\n","      <td>李天一已被安排出国</td>\n","      <td>李天一即将提前出狱？官方辟谣：仍在服刑！</td>\n","      <td>disagreed</td>\n","      <td>disagreed</td>\n","    </tr>\n","    <tr>\n","      <th>2491</th>\n","      <td>李天一已被安排出国</td>\n","      <td>北京市监狱管理局：李天一提前出狱消息不实系谣言，仍在监狱服刑</td>\n","      <td>disagreed</td>\n","      <td>disagreed</td>\n","    </tr>\n","    <tr>\n","      <th>2571</th>\n","      <td>沈阳两名女子偷孩子</td>\n","      <td>两名女子偷孩子 沈阳网警辟谣：假的！</td>\n","      <td>disagreed</td>\n","      <td>disagreed</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         text_a                          text_b      label  predicted\n","25      关于植物除甲醛     医学博士拆穿，市面那些关于装修除甲醛谣言,毫无科学依据  disagreed  disagreed\n","1605  李天一已被安排出国            李天一即将提前出狱？官方辟谣：仍在服刑！  disagreed  disagreed\n","2491  李天一已被安排出国  北京市监狱管理局：李天一提前出狱消息不实系谣言，仍在监狱服刑  disagreed  disagreed\n","2571  沈阳两名女子偷孩子              两名女子偷孩子 沈阳网警辟谣：假的！  disagreed  disagreed"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"mX35QgYML0LT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626314313785,"user_tz":-480,"elapsed":73,"user":{"displayName":"YS Shr","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvhvnYZyUI0w6BlE76qMYpohuCLbLbbLsd_SAcow=s64","userId":"15499537188569097981"}},"outputId":"76e955e8-7233-4064-f30d-03c91554d63b"},"source":["import numpy as np\n","\n","\n","text_a = \"李天一已被安排出国\"\n","text_b = \"李天一即将提前出狱？官方：是的！\"\n","word_pieces = [\"[CLS]\"] #開頭［CLS]\n","tokens_a = tokenizer.tokenize(text_a)#text_a 斷字\n","#print('tokens_a:', tokens_a)\n","word_pieces += tokens_a + [\"[SEP]\"]#[CLS]text_a已斷字\n","#print('word_pieces:', word_pieces)\n","len_a = len(word_pieces)  #len_a: 11\n","       \n","tokens_b = tokenizer.tokenize(text_b)\n","word_pieces += tokens_b + [\"[SEP]\"] ##[CLS]text_a已斷字[SEP]text_b\n","#print('word_pieces:', word_pieces)\n","len_b = len(word_pieces) - len_a #len_b: 17\n","#print('word_pieces length: ',len(word_pieces))\n","\n","\n","ids = tokenizer.convert_tokens_to_ids(word_pieces) #transfer word to vector \n","#print('ids:\\n',ids) #[101, 3330, 1921, 671, 2347, 6158,... 4328, 8043, 2135, 3175, 8038, 3221, 4638, 8013, 102], 共28個\n","\n","# c = np.array(ids)\n","#print('ids尚未經過unsqueeze前的shapes: ',c.shape) #result: (28,)一維\n","\n","tokens_tensor = torch.tensor(ids).unsqueeze(0)#unsqueeze再給定參數位置插入一維(28,)成二維 (1,28) 裡面放ids數字 \n","#小寫tensor吃標量\\向量但不吃維度\n","#print('tokens_tensor:',tokens_tensor)  \n","print('tokens_tensor.shape:',tokens_tensor.shape) #(1,28)\n","print(type(tokens_tensor)) \n","\n","\n","\"\"\"0與１\"\"\"\n","#torch.tensor的用途於將list 2 tensor, unsqueeze(0)則是增加dim, torch.long = torch.int64\n","\n","\n","#segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long) \n","#print('shape of segments_tensor without unsqueeze:',segments_tensor.shape) ----> torch.Size([28])\n","#ex. tensor([0, 0, 0, 0,...., 0, 0, 0, 0, 1, 1,....,1, 1])\n","\n","segments_tensor = torch.tensor([0] * len_a + [1] * len_b,dtype=torch.long).unsqueeze(0) #len_a =11, len_b = 17\n","#補上11個零的1D list與17個1的一為向量用加字號concate成list(1D)再用unsqueeze(0)轉成2D張量　#segment_tensor.shape = [1,28] 再unsqueeze to 張量(tensor)\n","print('shape of segments_tensor:',segments_tensor.shape)\n","##segments_tensor = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n","\n","\n","masks_tensors = torch.zeros(tokens_tensor.shape,dtype=torch.long) #([[1,28]], int 64)\n","print('Before mask: \\n',masks_tensors) #2D:28個0 tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n","\n","\n","masks_tensors = masks_tensors.masked_fill(tokens_tensor != 0, 1).unsqueeze(0) #若tokens_tensor 裡面數值(ids) !=0， 回傳1 ，unsqueeze [1, 28]轉3 dim張量torch.Size([1, 1, 28])\n","print('After mask: \\n',masks_tensors)\n","print('After mask shape: \\n',masks_tensors.shape)\n","\n","outputs = model(input_ids=tokens_tensor.to(device),token_type_ids=segments_tensor.to(device),attention_mask=masks_tensors.to(device)) #將所有最開始讀取數據時的tensor變量copy一份到device所指定的GPU上去，之後的運算都在GPU上進行。\n","#多CPU寫法 : device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#print('\\n outputs: ',outputs) \n","#outputs = SequenceClassifierOutput(loss=None, logits=tensor([[ 0.8479, -0.9631, -0.9965]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n","\n","logits = outputs[0] #取出[[ 0.8479, -0.9631, -0.9965]], device='cuda:0', grad_fn=<AddmmBackward>)\n","#print(f'\\n logits.data:{logits.data}') #取出機率值[[ 0.6375, -0.6297, -0.3883]], device='cuda:0'\n","\n","_, pred = torch.max(logits.data, 1) #return 每一行中最大值的那個元素，且返回其索引\n","\n","label_map = {0:'agreed', 1: 'disagreed', 2: 'unrelated'}\n","# print(f'\\n pred.cpu(): {pred.cpu()}')\n","print(f'\\n outputs:{outputs}')\n","print(label_map[pred.cpu().tolist()[0]]) #pred.cpu() = tensor([0]) 轉tensor to list\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tokens_tensor.shape: torch.Size([1, 28])\n","<class 'torch.Tensor'>\n","shape of segments_tensor: torch.Size([1, 28])\n","Before mask: \n"," tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0]])\n","After mask: \n"," tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","          1, 1, 1, 1, 1]]])\n","After mask shape: \n"," torch.Size([1, 1, 28])\n","\n"," outputs:SequenceClassifierOutput(loss=None, logits=tensor([[ 2.1363, -1.2171, -0.5522]], device='cuda:0', grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n","agreed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dVMtLg-TzuRC"},"source":[""]},{"cell_type":"code","metadata":{"id":"hrl96-BMFxKJ"},"source":[""],"execution_count":null,"outputs":[]}]}